#pragma once

#include <vector>
#include <memory>
#include <variant>
#include <map>
#include <unordered_map>
#include <set>
#include <unordered_set>

#include <vortex/core.hpp>

#include <vortex/driver/cuda/copy.hpp>

#include <vortex/memory.hpp>
#include <vortex/memory/teledyne.hpp> // always include to ensure variant is correct size

#include <vortex/engine/scan.hpp>
#include <vortex/engine/detail/session.hpp>
#include <vortex/engine/adapter.hpp>

#include <vortex/marker.hpp>

#include <vortex/scan.hpp>

#include <vortex/util/sync.hpp>

namespace vortex::engine {

    template<typename acquire_element_t_, typename process_element_t_, typename analog_element_t_, typename digital_element_t_>
    struct block_t {
        using acquire_element_t = acquire_element_t_;
        using process_element_t = process_element_t_;
        using analog_element_t = analog_element_t_;
        using digital_element_t = digital_element_t_;

        //
        // identification
        //

        using id_t = size_t;
        size_t id;
        time_t timestamp;
        counter_t sample;
        size_t length;

        //
        // scan markers
        //

        using marker_t = default_marker_t;
        std::vector<marker_t> markers;

        //
        // OCT data handling
        //

        // raw data
        using spectra_stream_t = std::variant<
#if defined(VORTEX_ENABLE_ALAZAR_GPU)
            sync::lockable<alazar::alazar_device_tensor_t<acquire_element_t>>,
#endif
            sync::lockable<teledyne::teledyne_cpu_view_t<acquire_element_t>>,
            sync::lockable<teledyne::teledyne_cuda_view_t<acquire_element_t>>,
            sync::lockable<cuda::cuda_device_tensor_t<acquire_element_t>>,
            sync::lockable<cuda::cuda_host_tensor_t<acquire_element_t>>
        >;
        std::vector<spectra_stream_t> spectra;

        // processed data
        using ascan_stream_t = std::variant<
            sync::lockable<cuda::cuda_device_tensor_t<process_element_t>>,
            sync::lockable<cuda::cuda_host_tensor_t<process_element_t>>
        >;
        std::vector<ascan_stream_t> ascans;

        //
        // synchronized I/O handling
        //

        // running index of each record
        using counter_stream_t = sync::lockable<cpu_tensor_t<counter_t>>;
        counter_stream_t counter;

        using scan_stream_t = sync::lockable<cuda::cuda_host_tensor_t<analog_element_t>>;

        // galvo target waveforms with various leads to compensate for hardware lag
        std::unordered_map<size_t, scan_stream_t> galvo_target;

        // on-time target and actual waveforms
        scan_stream_t sample_target, galvo_actual, sample_actual;

        // strobes generated by engine with various leads
        using strobe_stream_t = sync::lockable<cpu_tensor_t<digital_element_t>>;
        std::unordered_map<size_t, strobe_stream_t> strobes;

        auto streams(size_t lead_samples = 0) {
            return std::forward_as_tuple(counter, galvo_target.at(lead_samples), sample_target, galvo_actual, sample_actual, strobes.at(lead_samples));
        }
        auto streams(size_t lead_samples = 0) const {
            return std::forward_as_tuple(counter, galvo_target.at(lead_samples), sample_target, galvo_actual, sample_actual, strobes.at(lead_samples));
        }

        enum class stream_index_t : size_t {
            counter = 0,
            galvo_target,
            sample_target,
            galvo_actual,
            sample_actual,
            strobes
        };
    };

    template<typename block_t_, typename warp_t>
    struct engine_config_t {
        using block_t = block_t_;
        using adapter = engine::adapter<block_t>;

        //
        // buffering
        //

        size_t records_per_block = 1000;
        size_t blocks_to_allocate = 4;
        size_t preload_count = 2;

        //
        // acquisition control
        //

        size_t blocks_to_acquire = 0;
        size_t post_scan_records = 0;

        //
        // scanner info
        //

        warp_t scanner_warp{ scan::warp::none_t{} };
        size_t galvo_output_channels = 2;
        size_t galvo_input_channels = 2;

        //
        // lead control
        //

        typename block_t::digital_element_t lead_strobes = 0;
        marker::scan_boundary lead_marker{ 0, 0, 0, marker::scan_boundary::flags_t::none() };

        //
        // acquisition -> process graph
        //

        struct divide_t;
        struct cycle_t;
        using node = std::variant<divide_t, cycle_t, typename adapter::processor>;

        struct divide_t { std::vector<node> subgraph; };
        template<typename... Ns>
        static auto divide(Ns&... nodes) {
            divide_t d;
            _bind_to_graph(d.subgraph, nodes...);
            return d;
        }

        struct cycle_t { std::vector<node> subgraph; };
        template<typename... Ns>
        static auto cycle(Ns&... nodes) {
            cycle_t c;
            _bind_to_graph(c.subgraph, nodes...);
            return c;
        }

        using process_graph_t = node;

        template<typename A, typename... Ns, typename = typename std::enable_if_t<std::tuple_size_v<std::tuple<Ns...>> >= 2>>
        void add_acquisition(std::shared_ptr<A>& acquire, bool preload, bool master, Ns&&... nodes) {
            add_acquisition(acquire, preload, master, divide(nodes...));
        }
        template<typename A, typename P>
        void add_acquisition(std::shared_ptr<A>& acquire, bool preload, bool master, std::shared_ptr<P>& process) {
            add_acquisition(acquire, preload, master, node{ bind::processor<block_t>(process) });
        }
        template<typename A>
        void add_acquisition(std::shared_ptr<A>& acquire, bool preload, bool master, process_graph_t graph) {
            auto& a = acquisitions[bind::acquisition<block_t>(acquire)];

            // update options
            a.preload = preload;
            a.master = master;

            // append graph to any existing one by dividing
            std::visit(overloaded{
                [&](divide_t& n) {
                    n.subgraph.push_back(std::move(graph));
                },
                [&](auto& n) {
                    a.graph = divide(a.graph, graph);
                }
            }, a.graph);
        }

        struct acquire_config { bool preload, master; process_graph_t graph; };
        std::map<typename adapter::acquisition, acquire_config> acquisitions;

    protected:

        template<typename N, typename... Ns, typename = typename std::enable_if_t<std::is_same_v<std::decay_t<N>, node> || std::is_same_v<std::decay_t<N>, divide_t> || std::is_same_v<std::decay_t<N>, cycle_t>>>
        static void _bind_to_graph(std::vector<node>& subgraph, N& head, Ns&... rest) {
            subgraph.push_back(head);
            _bind_to_graph(subgraph, rest...);
        }
        template<typename P, typename... Ns>
        static void _bind_to_graph(std::vector<node>& subgraph, std::shared_ptr<P>& head, Ns&... rest) {
            subgraph.push_back(bind::processor<block_t>(head));
            _bind_to_graph(subgraph, rest...);
        }
        static void _bind_to_graph(std::vector<node>& subgraph) {}

    public:

        using format_graph_t = std::vector<typename adapter::formatter>;
        using endpoint_graph_t = std::vector<typename adapter::endpoint>;

        template<typename P, typename... Fs>
        void add_processor(const std::shared_ptr<P>& processor, const std::shared_ptr<Fs>&... formatters) {
            static_assert(sizeof...(Fs) > 0, "at least 1 formatter required");
            _bind_to_vector(processors[bind::processor<block_t>(processor)].graph, formatters...);
        }

        template<typename F, typename... EPs>
        void add_formatter(const std::shared_ptr<F>& formatter, const std::shared_ptr<EPs>&... endpoints) {
            static_assert(sizeof...(EPs) > 0, "at least 1 endpoint required");
            _bind_to_vector(formatters[bind::formatter<block_t>(formatter)].graph, endpoints...);
        }

        struct processor_config { format_graph_t graph; };
        std::map<typename adapter::processor, processor_config> processors;
        struct formatter_config { endpoint_graph_t graph; };
        std::map<typename adapter::formatter, formatter_config> formatters;

    protected:

        //template<typename A, typename G, typename T, typename... Ts>
        //auto _bind_to_vector(A& adapt, G& graph, T& head, Ts&... rest) {
        //    graph.push_back(adapt(head));
        //    _bind_to_vector(graph, rest...);
        //}
        //template<typename A, typename G>
        //auto _bind_to_vector(A&, G&) {}

        template<typename T, typename... Ts>
        static auto _bind_to_vector(format_graph_t& graph, T& head, Ts&... rest) {
            graph.push_back(bind::formatter<block_t>(head));
            _bind_to_vector(graph, rest...);
        }
        template<typename T, typename... Ts>
        static auto _bind_to_vector(endpoint_graph_t& graph, T& head, Ts&... rest) {
            graph.push_back(bind::endpoint<block_t>(head));
            _bind_to_vector(graph, rest...);
        }
        template<typename G>
        static auto _bind_to_vector(G&) {}

    public:

        struct io_config {
            // queue blocks before session starts
            bool preload = true;
            // delay start until preloading is complete
            bool master = false;
            // number of samples in advance of current sample to generate to compensate for delay
            size_t lead_samples = 0;
        };

        template<typename T>
        void add_io(std::shared_ptr<T>& io, bool preload = true, bool master = false, size_t lead_samples = 0) {
            add_io(io, { preload, master, lead_samples });
        }
        template<typename T>
        void add_io(std::shared_ptr<T>& io, io_config options) {
            ios[bind::io<block_t>(io)] = std::move(options);
        }

        std::map<typename adapter::io, io_config> ios;

        std::vector<strobe_t> strobes = {
            strobe::sample(0, 2),
            strobe::sample(1, 1000, strobe::polarity_t::high, 10),
            strobe::sample(2, 1000, strobe::polarity_t::low, 10),
            strobe::segment(3),
            strobe::volume(4)
        };

        void validate() {
            // XXX: assume config is validated
            // - check that all processors have inputs
            // - check that all formatters have inputs
            // - all processors take input from only one acquire
            // - no duplicate acquires/processors/formatters/endpoints -- maybe this is fine actually for endpoints
            // - every formatter receives input from exactly one processor in every process set
            // - consistent polarity for strobes (i.e., all strobes for a single channel have the same polarity)
            // - check that strobe lines will fit in datatype representing the strobes

            // require unique formatters for each processor
            for (const auto& [_, c] : processors) {
                std::set<typename adapter::formatter> unique_formatters;
                for (auto& f : c.graph) {
                    unique_formatters.insert(f);
                }

                if (c.graph.size() != unique_formatters.size()) {
                    throw std::runtime_error("repeated formatters for a single processor are not permitted");
                }
            }

            // require unique endpoints for each formatter
            for (const auto& [_, c] : formatters) {
                std::set<typename adapter::endpoint> unique_endpoints;
                for (auto& ep : c.graph) {
                    unique_endpoints.insert(ep);
                }

                if (c.graph.size() != unique_endpoints.size()) {
                    throw std::runtime_error("repeated endpoints for a single formatter are not permitted");
                }
            }
        }

    };

    namespace detail {

        template<typename block_t>
        struct engine_plan_t {
            using adapter = engine::adapter<block_t>;
            using spectra_stream_t = typename block_t::spectra_stream_t;
            using acquire_element_t = typename block_t::acquire_element_t;
            using spectra_stream_factory_t = typename adapter::spectra_stream_factory_t;
            using ascan_stream_factory_t = typename adapter::ascan_stream_factory_t;

            engine_plan_t(std::shared_ptr<spdlog::logger> log = nullptr)
                : _log(std::move(log)) {}

            using shape_t = typename adapter::shape_t;
            using stride_t = typename adapter::stride_t;
            using channels_t = std::vector<size_t>;

            //
            // configuration per component
            //

            using processor_sets = std::vector<std::vector<typename adapter::processor>>;

            struct acquire_plan {
                processor_sets rotation;
                size_t output_index;
            };
            struct process_plan {
                cuda::event_t start, done;
                size_t input_index, output_index;
            };
            struct format_plan {
                size_t format_index;
            };
            struct endpoint_plan {
                size_t endpoint_index;
            };

            std::map<typename adapter::acquisition, acquire_plan> acquire;
            std::map<typename adapter::processor, process_plan> process;
            std::map<typename adapter::formatter, format_plan> format;
            std::map<typename adapter::endpoint, endpoint_plan> endpoint;

            //
            // plans for allocating buffers associated with blocks
            //

            size_t allocate_spectra_buffer(bool elide, const typename adapter::acquisition& acquisition, spectra_stream_factory_t factory, const std::optional<cuda::device_t>& device, channels_t channels, shape_t shape, stride_t stride) {
                // generate buffer descriptor
                auto buffer = spectra_buffer{ acquisition, std::move(factory), device, std::move(channels), std::move(shape), std::move(stride) };

                // check if buffer is mandatory or not combinable
                auto it = std::find(spectra_buffers.begin(), spectra_buffers.end(), buffer);
                if (!elide || it == spectra_buffers.end()) {
                    // new buffer required
                    if (buffer.device) {
                        if (_log) { _log->debug("requesting [{}] buffer on device {} in block for spectra", shape_to_string(buffer.shape), *buffer.device); }
                    } else {
                        if (_log) { _log->debug("requesting [{}] buffer on host in block for spectra", shape_to_string(buffer.shape)); }
                    }

                    // add the buffer details for allocation
                    spectra_buffers.push_back(std::move(buffer));

                    // ensure a transfer stream exists for this device
                    if (device && transfer_streams.find(*device) == transfer_streams.end()) {
                        transfer_streams[*device];
                    }

                    return spectra_buffers.size() - 1;
                } else {
                    // buffer can be combined with an existing one
                    if (buffer.device) {
                        if (_log) { _log->debug("elided buffer on device {} in block for spectra", *device); }
                    } else {
                        if (_log) { _log->debug("elided buffer on host in block for spectra"); }
                    }
                    return std::distance(spectra_buffers.begin(), it);
                }
            }

            size_t allocate_ascan_buffer(ascan_stream_factory_t factory, const std::optional<cuda::device_t>& device, shape_t shape) {
                // all output buffers use the default stride
                stride_t stride;
                dense_stride(shape, stride);

                // each of these buffers is unique
                ascan_buffers.push_back({ std::move(factory), device, std::move(shape), std::move(stride) });
                if (device) {
                    if (_log) { _log->debug("requesting [{}] buffer on device {} in block for A-scans", shape_to_string(shape), *device); }
                } else {
                    if (_log) { _log->debug("requesting [{}] buffer on host in block for A-scans", shape_to_string(shape)); }
                }

                return ascan_buffers.size() - 1;
            }

            struct spectra_buffer {
                typename adapter::acquisition acquisition;

                spectra_stream_factory_t factory;
                std::optional<cuda::device_t> device;

                channels_t channels;
                shape_t shape;
                stride_t stride;

                // compare buffers except for prototype
                bool operator==(const spectra_buffer& o) const {
                    return acquisition == o.acquisition
                        && o.device == device
                        && equal(channels, o.channels)
                        && equal(shape, o.shape)
                        && equal(stride, o.stride);
                }
            };
            // NOTE: do not use set because the insertion order is not preserved so returned buffer indices will be wrong
            std::vector<spectra_buffer> spectra_buffers;

            struct ascan_buffer {
                ascan_stream_factory_t factory;
                std::optional<cuda::device_t> device;
                shape_t shape;
                stride_t stride;
            };
            std::vector<ascan_buffer> ascan_buffers;

            //
            // bookkeeping for queue allocation
            //

            size_t format_counter = 0;
            size_t endpoint_counter = 0;

            //
            // dedicated transfer streams per device
            //

            std::map<cuda::device_t, cuda::stream_t> transfer_streams;

            //
            // bookkeeping for IO delay correction
            //

            // sorted from smallest to largest
            std::vector<size_t> io_lead_samples;

            std::shared_ptr<spdlog::logger> _log;

        };
    }

    template<typename config_t_>
    class engine_t {
    public:

        using config_t = config_t_;
        using block_t = typename config_t::block_t;
        using adapter = engine::adapter<block_t>;
        using master_plan_t = detail::engine_plan_t<block_t>;

        using acquisition_t = typename adapter::acquisition;
        using processor_t = typename adapter::processor;
        using io_t = typename adapter::io;
        using formatter_t = typename adapter::formatter;
        using endpoint_t = typename adapter::endpoint;

        using acquire_element_t = typename block_t::acquire_element_t;
        using process_element_t = typename block_t::process_element_t;
        using analog_element_t = typename block_t::analog_element_t;
        using digital_element_t = typename block_t::digital_element_t;

        using process_sets = typename master_plan_t::processor_sets;

        using scan_queue_t = engine::scan_queue_t<typename block_t::scan_stream_t::element_t, typename block_t::marker_t>;

    public:

        engine_t(std::shared_ptr<spdlog::logger> log = nullptr)
            : _log(std::move(log)) {
            _scan_queue = std::make_shared<scan_queue_t>();
        }

        virtual ~engine_t() {
            try {
                stop();
            } catch(const std::exception&) {
                if (_log) { _log->warn("exception suppressed in engine destructor: {}", to_string(std::current_exception())); }
            }
        }

        void initialize(config_t config) {
            config.validate();
            std::swap(_config, config);

            _master_plan = master_plan_t(_log);
        }

        auto& scan_queue() {
            return _scan_queue;
        }
        const auto& scan_queue() const {
            return _scan_queue;
        }

    protected:

        //
        // acquisition -> processor -> format planning
        //

        auto _expand_acquire_graph(typename config_t::node& c) {
            std::set<processor_t> unique;

            std::function<process_sets(typename config_t::node&)> compute_sets = [&](typename config_t::node& c) {
                return std::visit(overloaded{
                    [&](processor_t& p) {
                        unique.insert(p);
                        return process_sets{ { p } };
                    },
                    [&](typename config_t::divide_t& d) {
                        // compute the subplans
                        std::vector<process_sets> subplans;
                        std::transform(d.subgraph.begin(), d.subgraph.end(), std::back_inserter(subplans), compute_sets);

                        // can safely assume that the subplans are each square in shape
                        std::vector<size_t> lengths;
                        std::transform(subplans.begin(), subplans.end(), std::back_inserter(lengths), [](auto& o) { return o.size(); });
                        auto lcm = std::reduce(lengths.begin(), lengths.end(), 1LL, [](const auto& a, const auto& b) { return std::lcm(a, b); });

                        // elongate all columns to have the same number of rows
                        for (auto& sp : subplans) {
                            sp.reserve(lcm);
                            size_t n = sp.size();
                            for (size_t i = 1; i < lcm / n; i++) {
                                std::copy_n(sp.begin(), n, std::back_inserter(sp));
                            }
                        }

                        // merge into combined plan (column stack)
                        process_sets plan;
                        plan.resize(lcm);
                        for (size_t i = 0; i < plan.size(); i++) {
                            for (auto& sp : subplans) {
                                std::move(sp[i].begin(), sp[i].end(), std::back_inserter(plan[i]));
                            }
                        }

                        return plan;
                    },
                    [&](typename config_t::cycle_t& c) {
                        // compute the plan
                        std::vector<process_sets> subplans;
                        std::transform(c.subgraph.begin(), c.subgraph.end(), std::back_inserter(subplans), compute_sets);

                        // can safely assume that the subplans are each square in shape
                        std::vector<size_t> lengths;
                        std::transform(subplans.begin(), subplans.end(), std::back_inserter(lengths), [](auto& o) { return o.front().size(); });
                        auto lcm = std::reduce(lengths.begin(), lengths.end(), 1LL, [](const auto& a, const auto& b) { return std::lcm(a, b); });

                        // elongate all rows to have the same number of columns
                        for (auto& sp : subplans) {
                            for (auto& row : sp) {
                                size_t n = row.size();
                                row.reserve(lcm);
                                for (size_t i = 1; i < lcm / n; i++) {
                                    std::copy_n(row.begin(), n, std::back_inserter(row));
                                }
                            }
                        }

                        // merge into combined plan (row stack)
                        process_sets plan;
                        plan.reserve(std::accumulate(subplans.begin(), subplans.end(), 0LL, [](const auto& a, const auto& b) { return a + b.size(); }));
                        for (auto& sp : subplans) {
                            std::move(sp.begin(), sp.end(), std::back_inserter(plan));
                        }

                        return plan;
                    }
                }, c);
            };

            return std::make_tuple(compute_sets(c), unique);
        }
    void _plan_acquire(const acquisition_t& acquire, typename config_t::node& nodes) {
            auto acquire_device = acquire.device();

            std::set<processor_t> unique_processors;
            auto& acquire_plan = _master_plan.acquire[acquire];

            // determine processor rotation
            std::tie(acquire_plan.rotation, unique_processors) = _expand_acquire_graph(nodes);

            // determine channel needs
            typename master_plan_t::channels_t channels(acquire.channels_per_sample());
            std::iota(channels.begin(), channels.end(), 0);

            // allocate output buffer on host or device, depending upon prototype
            acquire_plan.output_index = _master_plan.allocate_spectra_buffer(
                false, acquire,
                std::move(acquire.stream_factory()), acquire_device,
                channels, std::move(acquire.output_shape()), std::move(acquire.output_stride())
            );

            // set up transfer buffers
            for(auto& process : unique_processors) {
                auto& plan = _master_plan.process[process];
                auto process_device = process.device();

                if (acquire_device && process_device) {
                    // spectra on device

                    // ensure peer to peer access is available, if needed
                    if (*acquire_device != *process_device) {
                        cuda::peer_access(*process_device, *acquire_device, true);
                    }

                    // reuse the input buffer
                    plan.input_index = acquire_plan.output_index;

                } else if (acquire_device) {
                    // spectra device -> host

                    auto stream_factory = []() -> typename adapter::spectra_stream_t {
                        return sync::lockable<cuda::cuda_host_tensor_t<acquire_element_t>>();
                    };

                    // need a transfer buffer on host
                    // NOTE: if possible (device, channels, and shape match), allocate_spectra_buffer() will provide the same buffer to the acquisition output and processor input
                    plan.input_index = _master_plan.allocate_spectra_buffer(
                        true, acquire,
                        std::move(stream_factory), {},
                        channels, std::move(acquire.output_shape()), std::move(acquire.output_stride())
                    );

                } else if (process_device) {
                    // spectra host -> device

                    auto stream_factory = []() -> typename adapter::spectra_stream_t {
                        return sync::lockable<cuda::cuda_device_tensor_t<acquire_element_t>>();
                    };

                    // need a transfer buffer on this device
                    // NOTE: if possible (device, channels, and shape match), allocate_spectra_buffer() will provide the same buffer to the acquisition output and processor input
                    plan.input_index = _master_plan.allocate_spectra_buffer(
                        true, acquire,
                        std::move(stream_factory), process_device,
                        channels, std::move(acquire.output_shape()), std::move(acquire.output_stride())
                    );

                } else {
                    // spectra on host

                    // reuse the input buffer
                    plan.input_index = acquire_plan.output_index;
                }

                if (process_device) {
                    auto stream_factory = []() -> typename adapter::ascan_stream_t {
                        return sync::lockable<cuda::cuda_device_tensor_t<process_element_t>>();
                    };

                    // need an output buffer on device
                    plan.output_index = _master_plan.allocate_ascan_buffer(std::move(stream_factory), *process_device, process.output_shape());
                } else {
                    auto stream_factory = []() -> typename adapter::ascan_stream_t {
                        return sync::lockable<cuda::cuda_host_tensor_t<process_element_t>>();
                    };

                    // need an output buffer on device
                    plan.output_index = _master_plan.allocate_ascan_buffer(std::move(stream_factory), {}, process.output_shape());
                }
            }
        }

        //
        // format -> endpoint planning
        //

        std::optional<cuda::device_t> _choose_endpoint_device(const endpoint_t& endpoint) {
            // find preceding formatters
            std::set<formatter_t> formats;
            for (const auto& [f, v] : _config.formatters) {
                if (std::find(v.graph.begin(), v.graph.end(), endpoint) != v.graph.end()) {
                    formats.insert(f);
                }
            }
            if (formats.empty()) {
                throw std::invalid_argument("endpoint is not in formatter graph");
            }

            // find preceding processor output devices and accumulate them
            std::map<int, size_t> device_frequency;
            for (const auto& [p, fs] : _config.processors) {
                for (auto& f : formats) {
                    if (std::find(fs.graph.begin(), fs.graph.end(), f) != fs.graph.end()) {
                        auto device = p.device();
                        if (device) {
                            device_frequency[*device]++;
                        }
                    }
                }
            }
            if (device_frequency.empty()) {
                return {};
            }

            // find the most frequent device ID
            auto chosen_device = std::max_element(device_frequency.begin(), device_frequency.end(), [](auto& a, auto& b) { return a.second < b.second; })->first;
            return { chosen_device };
        }

    public:

        void prepare() {
            // intermediate buffers for acquire -> process -> format
            for (auto& [k, v] : _config.acquisitions) {
                _plan_acquire(k, v.graph);
            }

            // collect unique endpoints
            std::set<endpoint_t> unique_endpoints;
            for (const auto& [f, c] : _config.formatters) {
                _master_plan.format[f].format_index = _master_plan.format_counter++;

                for (auto& ep : c.graph) {
                    unique_endpoints.insert(ep);
                }
            }

            // intermediate buffers for format -> endpoint
            for (const auto& ep : unique_endpoints) {
                _master_plan.endpoint[ep].endpoint_index = _master_plan.endpoint_counter++;

                // notify endpoint to allocate appropriate resources
                auto device = _choose_endpoint_device(ep);
                ep.allocate(device, device);
            }

            // collect unique IO delay leads
            // NOTE: always include zero
            std::unordered_set<size_t> lead_samples = { 0 };
            for (const auto& [_, cfg] : _config.ios) {
                lead_samples.insert(cfg.lead_samples);
            }
            std::copy(lead_samples.begin(), lead_samples.end(), std::back_inserter(_master_plan.io_lead_samples));
            std::sort(_master_plan.io_lead_samples.begin(), _master_plan.io_lead_samples.end());

            _allocate_blocks();
        }

        void start() {
            std::unique_lock<std::mutex> lock(_mutex);

            if (!_session) {
                if (_log) { _log->info("starting engine"); }
                _session.emplace(_config, _master_plan, _blocks, *_scan_queue, _event_callback, _job_callback, _shutdown_exception, _log);
            }
        }

        void wait() const {
            std::unique_lock<std::mutex> lock(_mutex);

            if (_session) {
                _session->wait();
                _handle_error();
            }
        }
        auto wait_for(const std::chrono::high_resolution_clock::duration& timeout) const {
            std::unique_lock<std::mutex> lock(_mutex);

            if (_session) {
                auto finish = _session->wait_for(timeout);

                if (finish) {
                    _handle_error();
                }

                return finish;
            }
            return true;
        }

        auto done() const {
            std::unique_lock<std::mutex> lock(_mutex);

            if (_session) {
                return _session->done();
            }
            return true;
        }

        void shutdown(bool interrupt = false) {
            std::unique_lock<std::mutex> lock(_mutex);

            if (_session) {
                _session->quit(interrupt);
            }
        }

        void stop() {
            std::unique_lock<std::mutex> lock(_mutex);

            if (_session) {
                if (_log) { _log->info("stopping engine"); }

                // triggering the destructor will invoke quit()/wait() of the session
                _session.reset();

                _handle_error();
            }
        }

        struct engine_status {
            bool active;
            size_t dispatched_blocks = 0, inflight_blocks = 0;
            double dispatch_completion = 0, block_utilization = 0;
        };
        auto status() const {
            std::unique_lock<std::mutex> lock(_mutex);

            engine_status s;
            s.active = !!_session;

            if (_session) {
                auto ss = _session->status();
                s.dispatched_blocks = ss.dispatched;
                s.inflight_blocks = ss.inflight;

                if (_config.blocks_to_acquire > 0) {
                    s.dispatch_completion = double(s.dispatched_blocks) / _config.blocks_to_acquire;
                }
                s.block_utilization = double(s.inflight_blocks) / _config.blocks_to_allocate;
            }

            return s;
        }

        const auto& config() const {
            return _config;
        }

        void set_event_callback(event_callback_t&& callback) {
            std::unique_lock<std::mutex> lock(_mutex);

            _event_callback = std::forward<event_callback_t>(callback);
        }
        const auto& event_callback() const { return _event_callback; }

        void set_job_callback(job_callback_t&& callback) {
            std::unique_lock<std::mutex> lock(_mutex);

            _job_callback = std::forward<job_callback_t>(callback);
        }
        const auto& job_callback() const { return _job_callback; }

    protected:

        void _handle_error() const {
            if (_shutdown_exception) {
                // copy out and clear captured exception
                std::exception_ptr e;
                std::swap(e, _shutdown_exception);

                // throw
                std::rethrow_exception(e);
            }
        }

        template<typename stream_t, typename descriptor_t>
        void _allocate_stream(const descriptor_t& desc, stream_t& stream_) {
            // instantiate the stream
            stream_ = desc.factory();

            // allocate memory for streams backed by tensors
            std::visit([&](auto& stream) {
                if constexpr (vortex::is_tensor<decltype(stream)>) {

                    // ensure allocation is on correct device
                    if (desc.device) {
                        if constexpr (cuda::is_cuda_viewable<decltype(stream)>) {
                            // clear memory already allocated on the wrong device, if needed
                            if (stream.device() != *desc.device) {
                                stream.clear();
                            }
                        }
                        cuda::device(*desc.device);
                    }

                    // allocate the stream
                    stream.resize(desc.shape, desc.stride);

                }
            }, stream_);
        }

        void _allocate_blocks() {
            if (_log) { _log->debug("allocating {} blocks", _config.blocks_to_allocate); }

            _blocks.resize(_config.blocks_to_allocate);
            typename block_t::id_t id = 0;
            for (auto& block : _blocks) {
                block.id = id++;
                block.length = _config.records_per_block;

                block.counter.resize({ block.length, size_t(1) });

                for (auto n : _master_plan.io_lead_samples) {
                    block.galvo_target[n].resize({ block.length, _config.galvo_output_channels });
                    block.strobes[n].resize({ block.length, size_t(1) });
                }

                block.galvo_actual.resize({ block.length, _config.galvo_input_channels });

                block.sample_target.resize({ block.length, _config.galvo_output_channels });
                block.sample_actual.resize(block.galvo_actual.shape());

                block.spectra.resize(_master_plan.spectra_buffers.size());
                for (size_t i = 0; i < _master_plan.spectra_buffers.size(); i++) {
                    _allocate_stream(_master_plan.spectra_buffers[i], block.spectra[i]);
                }

                block.ascans.resize(_master_plan.ascan_buffers.size());
                for (size_t i = 0; i < _master_plan.ascan_buffers.size(); i++) {
                    _allocate_stream(_master_plan.ascan_buffers[i], block.ascans[i]);
                }
            }

            if (_log) { _log->debug("finished allocating blocks"); }
        }
        void _release_blocks() {
            _blocks.clear();
        }

        std::shared_ptr<scan_queue_t> _scan_queue;

        std::shared_ptr<spdlog::logger> _log;

        std::vector<block_t> _blocks;

        config_t _config;
        master_plan_t _master_plan;

        event_callback_t _event_callback;
        job_callback_t _job_callback;
        mutable std::exception_ptr _shutdown_exception;

        using session_t = detail::session_t<config_t, master_plan_t, scan_queue_t, block_t>;
        std::optional<session_t> _session;
        mutable std::mutex _mutex;

    };

}
